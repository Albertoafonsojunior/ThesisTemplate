\chapter{Algorithms}
\[
  d'(x) = Δ''(x) = g'''(x)
\]

\cleardoublepage
\section{Steffensen's method with linear mixing}
The selfconsistent solution for the superconducting gap~$Δ$ can be expressed
\[
  Δ = f(Δ),
\]
where the mean field $Δ = Δ(\B{r})$ in general is a function of position, and the function evaluation $f(Δ)$ implies solving both the Usadel equation and selfconsistency equation for the gap.
This fixpoint equation can be rewritten\\[-2ex]
\[
  d(Δ) \deq f(Δ) - Δ = 0,
\]
where the function $d(Δ)$ quantifies the difference after one fixpoint iteration for a gap~$Δ$.
This can be solved more efficiently using \eg Newton's method,
\[
  Δ_{n+1} = Δ_n - \frac{d(Δ_n)}{d'(Δ_n)}.
\]
In general, we do not know the exact form of $d'(Δ_n)$, so it has to be estimated numerically. 
For this, we may use a forward difference approximation,
\[
  d'(Δ_n) \approx \frac{d(Δ_{n+1}) - d(Δ_n)}{Δ_{n+1} - Δ_n},
\]
so that the Steffensen equation for convergence acceleration can be written
\[
  Δ_{n+3} = Δ_n - \frac{d(Δ_n)(Δ_{n+1} - Δ_n) }{d(Δ_{n+1}) - d(Δ_{n})}.
  \label{eq:steffensen}
\]
In the special case that we use regular fixpoint iterations between Steffensen boosts, we simply have $d(Δ_n) = Δ_{n+1} - Δ_n$, so this equation simplifies to
\[
  Δ_{n+3} = Δ_n - \frac{(Δ_{n+1} - Δ_n)^2 }{Δ_{n+2} - 2Δ_{n+1} + Δ_n} .
\]
Another alternative is to use \emph{linear mixing}.
We then perform fixpoint iterations as usual, but restrain the changes using a mixing factor~$α\in[0,1]$:\\[-2ex]
\[
  Δ_{n+1} = αf(Δ_n) + (1-α)Δ_n 
\]
Note that using an iteration pattern like this, $d(Δ_n) = Δ_{n+1} - Δ_n$ does not hold anymore.
However, by solving the equation above for $f(Δ_n) - Δ_n$, we just find that the corresponding expression just has an extra factor~$1/α$:
\[
  d(Δ_n) \deq f(Δ_n) - Δ_n = [Δ_{n+1} - Δ_n]/α.
\]
Going back to \cref{eq:steffensen}, we see that since there is only one factor~$d(\,\cdot\,)$ in the numerator and one in the denominator, the resulting factors~$α$ cancel out, leading to the same Steffensen iteration equation as before.

\clearpage
\section{Quasi-Newton least squares method}
Steffensen's method works very well for problems where only the magnitude of the order parameter~$|\Delta|$ changes between iterations.
However, it tends to struggle or even diverge if the phase~$\arg\phi$ changes as well, \ie for systems where a superconductor is subjected to a phase-bias or voltage-bias.
This leads to an extremely slow convergence for such systems, where the safe option is to simply turn the convergence acceleration off.

However, there exist some generalized convergence acceleration methods as well, which can handle fixpoint iteration equations for arbitrary real vectors.
Here, we will consider the relatively recently derived \emph{Quasi-Newton least squares} (\textsc{qnls}) method.\footnote{See papers by R.~Haelterman from 2009--2014, \eg \emph{J.\ Comp.\ App.\ Math.\ }\textbf{257}, 129 (2014).}
To keep things as simple and memory-efficient as possible, we focus on the evolution of the order parameter $\Delta \in \symbb{C}$ in a single spatial point~$\B{r}$, and disregard what happens elsewhere.

Since the \textsc{qnls} method was derived for functions~$\B{f}: \symbb{R}^n \rightarrow \symbb{R}^n$, we have to decompose the order parameter to a real vector $\B{Δ} \in \symbb{R}^2$ to use it,
\[
  \B{Δ} \deq
  \begin{bmatrix}
    \re Δ \\
    \im Δ \\
  \end{bmatrix}.
\]
In terms of this vector, we can formulate the mean-field problem as
\[
  \B{f}(\B{Δ}) = \B{Δ},
\]
where $\B{f}(\B{Δ})$ is a function that implies solving the Usadel equation and selfconsistency equation one time each.
The above fixed-point equation can also be reformulated as a typical root-finding equation
\[
  \B{d}(\B{Δ}) \deq \B{f}(\B{Δ}) - \B{Δ} = 0.
\]
These are the definitions we need in order to employ the \textsc{qnls} method.

As a direct generalization of Newton's method to vector-valued functions, we can now write down an iteration equation for the order parameter,
\[
  \B{Δ}_{n+1} = \B{Δ}_{n} - \B{J}^{-1}_n \B{f}(\B{Δ}_n).
\]
where $\B{J}_n$ is the $2\times2$ Jacobian of the system evaluated at~$\B{Δ}_n$.
Similarly to how Steffensen's method replaces the exact derivatives in one dimension with a finite-difference estimate, Haelterman \etal provides an equation for a least-squares estimate for the Jacobian based on previous iterations:
\[
  \B{J}_n \approx \B{W}_n(\B{V}_n^\trans \B{V}_n)^{-1} \B{V}_n^\trans - \B{1},
  \label{eq:jacobian-approx-rigorous}
\]
where $\B{1}$ is the $2\times2$ identity matrix.
The matrices $\B{V}_n$ and $\B{W}_n$ are defined via previous results for the order parameter and selfconsistency equation,
\begin{align}
  \B{V}_n &\deq
  \begin{bmatrix}
    \phantom{..}\quad\B{Δ}_1-\B{Δ}_0\quad\phantom{.} & \cdots &\quad\,\B{Δ}_{n-1}-\B{Δ}_{n-2}\quad\hspace{0.225em}\phantom{.}
  \end{bmatrix}, \\
  \B{W}_n &\deq
  \begin{bmatrix}
    \B{f}(\B{Δ}_1)-\B{f}(\B{Δ}_0) & \cdots & \B{f}(\B{Δ}_{n-1})-\B{f}(\B{Δ}_{n-2})
  \end{bmatrix}.
\end{align}
In principle, it might be possible to run a Newton iteration as described above \emph{every} iteration.
However, at least for how I have implemented the Usadel solver, it is important for the stability of the convergence to have periods with regular fixpoint iterations inbetween the boosts.
Let us now say that we wish to construct the matrices $\B{V}_n$ and $\B{W}_n$ from data generated by such fixpoint iterations, following the scheme $\B{Δ}_{n+1} = \B{f}(\B{Δ})$.
In this case, we see that the matrices above have the exact same structure, except that all indices $n$ are shifted by one iteration!
Thus, if we define a short-notation for the history of the order parameter through previous iterations
\[
  \B{d}_{i,j} \deq 
  \begin{bmatrix}
    \B{Δ}_{i} - \B{Δ}_{i-1} & \cdots & \B{Δ}_{j} - \B{Δ}_{j-1}
  \end{bmatrix},
\]
then the matrices $\B{V}_n = \B{d}_{1,n-1}$ and $\B{W}_n = \B{d}_{2,n}$.
This yields a simplified least-squares estimate for the Jacobian expressed entirely via these differences,\\[-2ex]
\[
  \B{J}_n \approx \B{d}_{2,n} \big( \B{d}_{1,n-1}^\trans \B{d}_{1,n-1} \big)^{-1} \B{d}_{1,n-1}^\trans - \B{1}.
  \label{eq:jacobian-approx-approx}
\]
Note that the matrices $\B{d}_{1,n-1}$ and $\B{d}_{2,n}$ have dimensions $2 \times (n-1)$, so that the Jacobian ends up with the dimensions $2\times2$ as expected.

To summarize: the proposed iteration scheme for systems with a phase-winding consists of a number of simple and stable fixed-point iterations,\\[-2ex]
\[
  \B{Δ}_{n+1} = \B{f}(\B{Δ}_n),
\]
followed by occasional convergence boosts using an \textsc{qnls} method,
\[
  \B{Δ}_{n+1} = \B{Δ}_{n} - \B{J}^{-1}_n \B{f}(\B{Δ}_n),
\]
where the Jacobian $\B{J}$ is found using \cref{eq:jacobian-approx-approx}.
How often one should perform these boosts to optimize convergence time without sacrificing any numerical stability should be determined empirically.
Comparing with our results using Steffensen's method, one might expect that at least 7 fixed-point iterations between boosts should be required.


\clearpage


\section{Critical temperature calculations}
\section{Steffensen's method}

\clearpage
\section{Interpolation}
When solving the nonequilibrium problem, the problem was formulated as:
\begin{align}
  \begin{pmatrix}
    \BC{H}\phantom{'} \\
    \BC{H}' 
  \end{pmatrix}'
  =
  -
  \begin{pmatrix}
    \B{0} & 
    \B{1} \\
    \BC{M}^{-1} \big[ (\partial_z\BC{M}) + \BC{Q} \big] &
    \BC{M}^{-1} \big[ (\partial_z\BC{Q}) + \BC{K} \big]
  \end{pmatrix}
  \begin{pmatrix}
    \BC{H}\phantom{'} \\
    \BC{H}' 
  \end{pmatrix}
\end{align}
One nice thing about this formulation, is that we have an explicit expression for the Jacobian $\B{J}$ of the problem, i.e. the $16\times16$ proportionality matrix in the equation above, which does not depend on the distribution function~$\BC{H}$.

However, each of the matrices $\BC{M}$, $\BC{Q}$, $\BC{K}$ are functions of the equilibrium propagators $\U{G}^R$ and $\U{G}^A$, which again depend on position~$z$.
This means that the Jacobian~$J(z)$ is position-dependent, and can only be explicitly calculated at the discretized positions where the equilibrium propagators are known.
The problem is that the differential equation solver needs to be able to calculate the Jacobian at arbitrary positions, which means that some form of interpolation is needed here.
My first approach was a simple linear interpolation.
That was unstable and crashed, since the numerical solver is sensitive to the \emph{derivative} of the Jacobian, which in this case becomes discontinuous.
My next approach was to try using the \texttt{pchip}-library (Piecewise Hermitian Cubic Interpolation) that I've used for cubic interpolations of e.g. the gap in the past.
However, this library only operates on scalar functions, and calling it $16^2 = 256$ separate times to interpolate the Jacobian at each point turned out to be really inefficient.
My solution was therefore to implement such a cubic matrix interpolator myself, in order to speed up the nonequilibrium solver.

\clearpage
\section{Kinetic equation}
As we have shown before...



\clearpage
\section{Boundary condition}
As we showed in the manuscript with Tom [Eq.~(31--32)], the boundary conditions at a spin-active interface can be written as follows:
\begin{align}
  \BC{J}_a &= \BC{C}_{aa} \BC{H}_a - \BC{C}_{ab} \BC{H}_b \\
  \BC{J}_b &= \BC{C}_{bb} \BC{H}_b - \BC{C}_{ba} \BC{H}_a 
\end{align}
The minus-sign on the right-hand sides appears because this is derived from a commutator. 
This sign is just a convenient convention -- in comparison to Tom's notation, this gets rid of the signs in Eq.~(36) of the arXiv manuscript.
Note that both the boundary conditions above follow the same pattern:
\begin{align}
  \BC{J} &= \BC{C} \BC{H} - \BC{C}'\BC{H}'
\end{align}
Here, the unprimed quantities refer to ``this'' side of the interface -- i.e. where the current $\BC{J}$ is calculated -- while the primed ones refer to ``the other'' side.

If we for now ignore spin-orbit coupling, we can substitute $\BC{J} = \BC{M}\partial\BC{H} + \BC{QH}$ into the equation above, and thus get the boundary condition:
\begin{align}
  \BC{M}\partial\BC{H} + \BC{QH} = \BC{C} \BC{H} - \BC{C}'\BC{H}'
\end{align}
This is equivalent to Eq.~(40) in the arXiv manuscript, but fixes a sign error in the $\BC{Q}$-term. 
This boundary condition can also be written in matrix form:
\begin{align}
  \BC{M}\partial\BC{H} + [\BC{Q}-\BC{C}]\BC{H} + \BC{C}'\BC{H}' = 0
\end{align}
Finally, this can be rewritten in matrix form as:
\begin{align}
  \begin{pmatrix}
    \BC{Q}-\BC{C} & \BC{M} \\
  \end{pmatrix}
  \begin{pmatrix}
    \BC{H} \\
    \partial\BC{H}
  \end{pmatrix}
  +
  \begin{pmatrix}
    \BC{C}' & 0 \\
  \end{pmatrix}
  \begin{pmatrix}
    \BC{H} \\
    \partial\BC{H}
  \end{pmatrix}
  &= 0
\end{align}
In other words: if we define the 16-element state vectors $\B{u} = [\BC{H}; \partial\BC{H}]$ and $\B{u}\ = [\BC{H}'; \partial\BC{H}']$, and the $8\times16$ matrices $\B{A} = [\BC{Q}-\BC{C}, \BC{M} ]$ and $\B{B} = [-\BC{C}', 0]$ that operate on these, then the boundary condition can be written:
\begin{align}
  \B{A}\B{u} - \B{B}\B{u}' = 0 .
\end{align}
\clearpage
These boundary conditions have a number of nice properties:
\begin{itemize}
  \item 
    The coefficients $\B{A}$ and $\B{B}$ only depend on equilibrium properties.
    This means that they only have to be calculated once, and evaluating the boundary condition is reduced to a simple matrix multiplication.
  \item 
    The equation has the form $\B{F}(\B{u},\B{u}') = 0$.
    This is perfect since I use a numerical solver that works by minimizing a user-specified residual.
  \item
    The Jacobian $J_{ij} = \partial F_i/\partial u_j$ of the boundary condition is simply the matrix $\B{A}$.
    Since I use a numerical solver that can use the Jacobian of the boundary condition to speed up convergence, this is quite useful.
\end{itemize}
In the case of transparent boundary conditions, the problem may again be formulated in the same way as above, but with Jacobians $\B{A} = \B{B} = [\B{1},\B{0}]$.
